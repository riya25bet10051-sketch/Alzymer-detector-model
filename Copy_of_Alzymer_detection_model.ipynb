{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPHTKlkVEovQi0IJ0r5znnL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/riya25bet10051-sketch/Alzymer-detector-model/blob/main/Copy_of_Alzymer_detection_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from  google.colab import files\n",
        "uploded = files.upload()"
      ],
      "metadata": {
        "id": "42p6XOk8ddGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip \"archive (1).zip\""
      ],
      "metadata": {
        "id": "fGTeRv7vil2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets pandas pillow tensorflow"
      ],
      "metadata": {
        "id": "Z2lBOXryiyoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from datasets import load_dataset\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import io\n",
        "\n",
        "# 1. Define the paths to your unzipped files\n",
        "# Based on your screenshot, the folder name is \"Alzheimer MRI Disease Classification Dataset\"\n",
        "base_path = \"Alzheimer MRI Disease Classification Dataset/Data\"\n",
        "train_file = os.path.join(base_path, \"train-00000-of-00001*\") # * captures the full hash\n",
        "test_file = os.path.join(base_path, \"test-00000-of-00001*\")\n",
        "\n",
        "print(\"Loading dataset from files...\")\n",
        "\n",
        "# 2. Load using the Hugging Face datasets library\n",
        "# We use shell globbing (the *) to match the long file names automatically\n",
        "dataset = load_dataset(\"parquet\", data_files={\"train\": train_file, \"test\": test_file})\n",
        "\n",
        "print(\"Dataset loaded successfully!\")\n",
        "print(dataset)"
      ],
      "metadata": {
        "id": "bx_UbzS9lLOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SIZE = 128\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "def preprocess_data(examples):\n",
        "    # This function processes a batch of raw data\n",
        "    images = []\n",
        "    for img_data in examples['image']:\n",
        "        # Check if image is already a dict (common in some parquet exports) or raw bytes\n",
        "        if isinstance(img_data, dict) and 'bytes' in img_data:\n",
        "            img_bytes = img_data['bytes']\n",
        "        else:\n",
        "            img_bytes = img_data # Assume it's direct bytes or PIL object\n",
        "\n",
        "        # Convert bytes to PIL Image\n",
        "        if not isinstance(img_bytes, Image.Image):\n",
        "            image = Image.open(io.BytesIO(img_bytes)).convert(\"RGB\")\n",
        "        else:\n",
        "            image = img_bytes.convert(\"RGB\") # It's already a PIL image\n",
        "\n",
        "        # Resize and Normalize\n",
        "        image = image.resize((IMG_SIZE, IMG_SIZE))\n",
        "        image = np.array(image) / 255.0 # Normalize to 0-1\n",
        "        images.append(image)\n",
        "\n",
        "    return {\"pixel_values\": images, \"label\": examples['label']}\n",
        "\n",
        "# Apply the preprocessing to the dataset\n",
        "print(\"Processing images... this might take a minute.\")\n",
        "processed_dataset = dataset.map(preprocess_data, batched=True, batch_size=32, remove_columns=[\"image\"])\n",
        "\n",
        "# Convert to TensorFlow compatible format\n",
        "train_ds = processed_dataset[\"train\"].to_tf_dataset(\n",
        "    columns=\"pixel_values\", label_cols=\"label\", batch_size=BATCH_SIZE, shuffle=True\n",
        ")\n",
        "test_ds = processed_dataset[\"test\"].to_tf_dataset(\n",
        "    columns=\"pixel_values\", label_cols=\"label\", batch_size=BATCH_SIZE, shuffle=False\n",
        ")\n",
        "\n",
        "print(\"Data is ready for training.\")"
      ],
      "metadata": {
        "id": "rsJpy6-rlWBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers, models\n",
        "\n",
        "def create_model():\n",
        "    model = models.Sequential([\n",
        "        # Input Layer\n",
        "        layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3)),\n",
        "\n",
        "        # First Convolutional Block\n",
        "        layers.Conv2D(32, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "        # Second Convolutional Block\n",
        "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "        # Third Convolutional Block\n",
        "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "        # Flatten and Dense Layers\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dropout(0.5), # Helps prevent overfitting\n",
        "\n",
        "        # Output Layer (4 classes: Mild, Moderate, Non, Very Mild)\n",
        "        layers.Dense(4, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    return model\n",
        "\n",
        "model = create_model()\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "Zcr9eyh9llfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train for 10 epochs\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=test_ds,\n",
        "    epochs=10\n",
        ")\n",
        "\n",
        "# Plot accuracy\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['accuracy'], label='accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([0, 1])\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2q2ZZWB5ltiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Initialize lists to store results\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "print(\"Generating predictions... this may take a moment.\")\n",
        "\n",
        "# Iterate through the test dataset batch by batch\n",
        "for images, labels in test_ds:\n",
        "    # Make predictions on the current batch\n",
        "    preds = model.predict(images, verbose=0)\n",
        "\n",
        "    # Convert probabilities to class index (0, 1, 2, or 3)\n",
        "    predicted_classes = np.argmax(preds, axis=1)\n",
        "\n",
        "    # Store the results\n",
        "    y_pred.extend(predicted_classes)\n",
        "    y_true.extend(labels.numpy())\n",
        "\n",
        "print(\"Predictions complete.\")"
      ],
      "metadata": {
        "id": "qmWRTf5QC0Ic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "a8rqtSJQFv58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class_names = ['Mild Demented', 'Moderate Demented', 'Non Demented', 'Very Mild Demented']\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=class_names,\n",
        "            yticklabels=class_names)\n",
        "\n",
        "plt.title('Confusion Matrix')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OATtTtHUDekY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a batch of images\n",
        "for images, labels in test_ds.take(1):\n",
        "    predictions = model.predict(images)\n",
        "    predicted_class = np.argmax(predictions[0])\n",
        "    true_class = labels[0].numpy()\n",
        "\n",
        "    classes = ['Mild Demented', 'Moderate Demented', 'Non Demented', 'Very Mild Demented']\n",
        "\n",
        "    print(f\"True Class: {classes[true_class]}\")\n",
        "    print(f\"Predicted Class: {classes[predicted_class]}\")\n",
        "\n",
        "    # Show the image\n",
        "    plt.imshow(images[0])\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "9GLBCeEKEEjY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}